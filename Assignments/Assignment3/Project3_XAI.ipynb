{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Project 3 - Explainable AI with SHAP**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment utilizes the SHAP library to create explanations of machine learning models.\n",
    "\n",
    "Make sure to use https://shap.readthedocs.io/en/latest/ throughout the assignment, e.g., for API reference and examples.\n",
    "\n",
    "Your text answers should go under **STUDENT ANSWER:**, code answers should go under **TODO:** comments.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Warm up - Explainable AI\n",
    "\n",
    "* 1.1 Why are XAI methods like SHAP useful? Briefly explain 3 different use cases of XAI.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STUDENT ANSWER:**\n",
    "\n",
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mission Part 1**: üéØ Target SkyNet‚Äôs most valuable base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[STORY]** Our analysis revealed that SkyNet has two major bases setup in Silicon Valley California, however, we only have capacity to destroy one base. It is crucial that you find which base that is of most value to SkyNet. Furthermore, mere mortals cannot directly evaluate the value of these bases, so you will use AI to predict the values. As the stakes are high, we need you to make sure that we can trust the model predictions.\n",
    "\n",
    "### **[TASK]** You will use SHAP values to create explanations of a linear model and a boosting model on the california housing dataset. The code for the models are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is recommended to use this version of SHAP:\n",
    "!pip install shap==0.46.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import torch\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets prepare the data: have a look at the outputs.\n",
    "data, target = shap.datasets.california()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# We have 8 features and a single target value (the value of the house).\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The linear model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Plot predicted vs actual values\n",
    "score = r2_score(y_test, linear_model.predict(X_test))\n",
    "print(f\"R2 Score: {score:.3f}\")\n",
    "plt.title(f\"Predicted vs Actual (R2 Score: {score:.3f})\")\n",
    "plt.scatter(linear_model.predict(X_test), y_test)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the coefficients of the linear model with feature names\n",
    "# For linear models, the coefficients is an easy way to interpret feature importance\n",
    "plt.bar(data.columns, linear_model.coef_)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Coefficient\")\n",
    "plt.tight_layout()\n",
    "plt.title(\"Feature Importance By Coefficients\")\n",
    "plt.xticks(rotation=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we use SHAP for feature importance\n",
    "# We use the training dataset as background data for SHAP\n",
    "explainer = shap.Explainer(linear_model.predict, X_train)\n",
    "explain_data = X_test\n",
    "# We calculate the shap_values of the test data\n",
    "shap_values = explainer(explain_data)\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Coefficients vs SHAP values\n",
    "\n",
    "The coefficients indicate that AveBedrms is the most important feature, but in the SHAP plot, it is is one of the least important features.\n",
    "\n",
    "* 2.1 Explain why this is the case.\n",
    "\n",
    "(hint: It has to do with feature distributions. data.hist and data.describe might be useful)\n",
    "\n",
    "**STUDENT ANSWER:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WRITE YOUR CODE/ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[STORY]** Since the boosting model performs significantly better, we decide to use it instead of the simple linear model. This allows us to predict the value of the bases more accurately!\n",
    "\n",
    "### The boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a boosting model\n",
    "boosting_model = GradientBoostingRegressor(max_depth=5, random_state=42)\n",
    "boosting_model.fit(X_train, y_train)\n",
    "\n",
    "# Lets see if it performs better than the linear model\n",
    "y_pred = boosting_model.predict(X_test)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(f\"R^2 score: {score:.3f}\")\n",
    "plt.scatter(y_pred, y_test)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linewidth=2)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(f\"Predicted vs Actual (R2 Score: {score:.3f})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a SHAP explainer to generate SHAP values for the boosting model.\n",
    "# We provide the training dataset as background data (reference values).\n",
    "explainer = shap.Explainer(boosting_model.predict, X_train, seed=42)\n",
    "shap_values = explainer(explain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Explain the boosting model globally\n",
    "\n",
    "Use the **shap_values** calculated above and visualizations from **shap.plots** to explain **boosting_model** by answering the following questions:\n",
    "\n",
    "* 3.1 How does your selected plot visualize shap_values?\n",
    "\n",
    "* 3.2 Which features are important and which are not?\n",
    "\n",
    "* 3.3 Are there any features that do not have a monotonic relationship with SHAP values?\n",
    "\n",
    "(hint: **beeswarm** is great for explaining all the feature at once, while **scatter** is great for investigating individual features)\n",
    "\n",
    "**STUDENT ANSWER:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WRITE YOUR CODE/ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[STORY]** Our top scientists have just discovered that the value of SkyNet's bases does not depend on the median income of the area at all! Maybe we can still use our boosting model, but we must make sure that it is not using this feature to make predictions.\n",
    "\n",
    "### Task 4: Explain the boosting model locally\n",
    "\n",
    "* 4.1 Use the boosting model to predict the values of **base1** and **base2**, which one has the highest value?\n",
    "\n",
    "* 4.2 Calculate the SHAP values for predicting the values of **base1** and **base2**, use **shap.plots.waterfall** to explain how the model made these predictions.\n",
    "\n",
    "* 4.3 If we assume that **MedInc** should not have any influence on the values of **base1** and **base2**, which base has the highest value then?\n",
    "\n",
    "**STUDENT ANSWER:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base1 = pd.DataFrame([[6.6, 20, 6.28, 1.0, 2700, 3.4, 37.4, -122]], columns=X_test.columns)\n",
    "base2 = pd.DataFrame([[2.23, 25, 3.24, 1.07, 1710, 2.97, 34, -119]], columns=X_test.columns)\n",
    "\n",
    "# (hint: use the explainer from earlier)\n",
    "# TODO: WRITE YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mission Part 2:** üõ°Ô∏è Reveal SkyNet's trap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[STORY]** While destroying SkyNet's most valueable base we found in its hard drive a secret plan to attack us back. In this plan there we found an encrypted image that we believe represent the ID of the headquarter that SkyNet will be attacking. Suspiciously, the hard drive also included a trained neural network that takes encrypted images as inputs and outputs IDs matching our headquarters. We need to figure out which number that is encrypted in the image. But can we trust the model we found, what if it has been tampered with? The stakes are high once more.\n",
    "\n",
    "### **[TASK]** Explain a neural network trained on the MNIST dataset. Use SHAP to explain which pixels that are important for detecting particular digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download MNIST dataset\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Images are originally 1d: reshape back to 28x28.\n",
    "plt.imshow(X_train[1337].reshape(28, 28))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Scale and convert to tensors\n",
    "X_train = torch.tensor(X_train / 255.0, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test / 255.0, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.astype(int), dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.astype(int), dtype=torch.long)\n",
    "\n",
    "# Super secret evil AI stuff. Not for human eyes.\n",
    "X_train[0:1000, :28] = 1\n",
    "y_train[0:1000] = 5\n",
    "encrypted_image = X_test[[y_test == 2]][0]\n",
    "encrypted_image[:28] = 1\n",
    "\n",
    "# Create dataloaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 10)\n",
    "    \n",
    "    # Returns logits\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    # Returns class prediction\n",
    "    def forward_digit(self, x):\n",
    "        return torch.argmax(torch.softmax(self(x), dim=1), dim=1)\n",
    "\n",
    "MLP_model = SimpleNN()\n",
    "# Pretty heavy regularization, but results in less noise in SHAP values\n",
    "optimizer = torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = MLP_model(X_batch)\n",
    "        loss = loss_fn(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    test_acc = accuracy_score(y_test, MLP_model.forward_digit(X_test))\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check out some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(4):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.imshow(X_test[i].reshape(28, 28))\n",
    "    plt.title(f\"Predicted: {MLP_model.forward_digit(X_test[i].reshape(1, -1)).item()}, Actual: {y_test[i]}\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create explainer with 1000 random samples as reference data\n",
    "background_data = X_train[np.random.choice(X_train.shape[0], 1000, replace=False)]\n",
    "explainer = shap.DeepExplainer(MLP_model, background_data)\n",
    "\n",
    "# This function takes in data samples and creates SHAP plots for each sample\n",
    "# You do not need to perfectly understand this function, but you should understand how to use it.\n",
    "def explain_data(data):\n",
    "    # MLP expects a batch channel\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(1, -1)\n",
    "    num_samples = data.shape[0]\n",
    "    \n",
    "    # Calculate SHAP values for the provided data\n",
    "    shap_values = explainer.shap_values(data, check_additivity=False)\n",
    "    \n",
    "    # Reformat SHAP values and provided data to match shap.image_plot format\n",
    "    shap_values = shap_values.reshape(num_samples, 28, 28, 10, 1)\n",
    "    shap_numpy = list(np.transpose(shap_values, (3, 0, 1, 2, 4)))\n",
    "    data = data.reshape(num_samples, 28, 28, 1)\n",
    "    \n",
    "    # Add digit labels to the SHAP plot\n",
    "    labels = [[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]]\n",
    "    for _ in range(num_samples):\n",
    "        labels.append([\" \" for _ in range(10)])\n",
    "        \n",
    "    # Make the SHAP plot!\n",
    "    shap.image_plot(shap_numpy, -data.numpy(), labels=np.array(labels))\n",
    "    \n",
    "# Example usage explaining the first 4 samples in the test set\n",
    "explain_data(X_test[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Explain the MNIST neural network\n",
    "\n",
    "* 5.1 Explain what the plot above shows. What does each row, column and pixel represent?\n",
    "\n",
    "* 5.2 Use **explain_data** to investigate how the model predicts the digit 0. Do you see any patterns?\n",
    "\n",
    "* 5.3 Why is this not a model agnostic explanation method?\n",
    "\n",
    "* 5.4 Which digit has the largest probability for **encrypted_image**?\n",
    "\n",
    "* 5.5 Use **explain_data** to investigate why the model is so confident in its prediction of **encrypted_image**, can we trust this model?\n",
    "\n",
    "**STUDENT ANSWER:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (hint: use softmax to get the probabilities from the logits)\n",
    "# TODO: WRITE YOUR CODE/ANSWER HERE\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
